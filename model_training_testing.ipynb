{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "!pip install -q pycocotools\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install -q pandas geopandas osmnx folium tqdm\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import osmnx as ox\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "from shapely.geometry import Polygon, Point\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# Detectron2 Imports\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultTrainer, DefaultPredictor, HookBase\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.evaluation import COCOEvaluator, SemSegEvaluator, inference_on_dataset\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip Data (Adjust path as needed)\n",
        "if not os.path.exists(\"/content/train\"):\n",
        "    !unzip -q \"/content/drive/My Drive/data.zip\" -d /content/"
      ],
      "metadata": {
        "id": "sIcVyaMAQx_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration & Registration ---\n",
        "register_coco_instances(\"train\", {}, \"/content/train/_annotations.coco.json\", \"/content/train\")\n",
        "register_coco_instances(\"valid\", {}, \"/content/valid/_annotations.coco.json\", \"/content/valid\")\n",
        "\n",
        "# Define Semantic Segmentation Helper for Custom Evaluator\n",
        "def load_semseg_dicts(image_dir, mask_dir):\n",
        "    dataset_dicts = []\n",
        "    for fname in sorted(os.listdir(image_dir)):\n",
        "        if not fname.endswith((\".png\", \".jpg\")): continue\n",
        "        record = {\n",
        "            \"file_name\": os.path.join(image_dir, fname),\n",
        "            \"sem_seg_file_name\": os.path.join(mask_dir, fname),\n",
        "            \"image_id\": fname,\n",
        "        }\n",
        "        dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "# --- Custom Classes for Training Monitoring ---\n",
        "class IOUEvaluator(SemSegEvaluator):\n",
        "    def __init__(self, dataset_name, distributed=True, output_dir=None):\n",
        "        super().__init__(dataset_name, distributed, output_dir)\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self._scores = []\n",
        "    def process(self, inputs, outputs):\n",
        "        for input_data, output in zip(inputs, outputs):\n",
        "            pred_mask = output['instances'].pred_masks.float().sum(dim=0) > 0\n",
        "            gt_mask = input_data['sem_seg'].cpu().numpy()\n",
        "            # Simple IoU calculation\n",
        "            intersection = np.logical_and(pred_mask.cpu().numpy(), gt_mask).sum()\n",
        "            union = np.logical_or(pred_mask.cpu().numpy(), gt_mask).sum()\n",
        "            iou = intersection / union if union > 0 else 0.0\n",
        "            self._scores.append(iou)\n",
        "    def evaluate(self):\n",
        "        return {\"sem_seg\": {\"IoU\": np.mean(self._scores) if self._scores else 0.0}}\n",
        "\n",
        "class CocoTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "        if output_folder is None: output_folder = os.path.join(cfg.OUTPUT_DIR, \"coco_eval\")\n",
        "        return COCOEvaluator(dataset_name, cfg, distributed=False, output_dir=output_folder)\n",
        "\n",
        "class IOUHook(HookBase):\n",
        "    def __init__(self, trainer, val_loader):\n",
        "        self.trainer = trainer\n",
        "        self.val_loader = val_loader\n",
        "        self.best_metric = -1\n",
        "    def after_step(self):\n",
        "        if (self.trainer.iter + 1) % self.trainer.cfg.TEST.EVAL_PERIOD == 0:\n",
        "            metrics = inference_on_dataset(self.trainer.model, self.val_loader, IOUEvaluator(\"validsemseg\"))\n",
        "            val_iou = metrics[\"sem_seg\"][\"IoU\"]\n",
        "            print(f\"[Iter {self.trainer.iter}] Validation IoU: {val_iou:.4f}\")\n",
        "            if val_iou > self.best_metric:\n",
        "                self.best_metric = val_iou\n",
        "                torch.save(self.trainer.model.state_dict(), os.path.join(self.trainer.cfg.OUTPUT_DIR, \"best_model.pth\"))\n",
        "\n",
        "# --- Run Training ---\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.DATASETS.TRAIN = (\"train\",)\n",
        "cfg.DATASETS.TEST = (\"valid\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 4000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
        "cfg.TEST.EVAL_PERIOD = 500\n",
        "cfg.OUTPUT_DIR = \"/content/output/\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Register dummy semseg datasets for the hook\n",
        "DatasetCatalog.register(\"validsemseg\", lambda: load_semseg_dicts(\"/content/valid\", \"/content/validmask\")) # Ensure masks exist or skip hook\n",
        "MetadataCatalog.get(\"validsemseg\").set(evaluator_type=\"sem_seg\", ignore_label=255)\n",
        "\n",
        "trainer = CocoTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "# Note: Ensure valid_loader is built if using IOUHook, otherwise remove hook\n",
        "# val_loader = build_detection_test_loader(cfg, \"validsemseg\")\n",
        "# trainer.register_hooks([IOUHook(trainer, val_loader)])\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Tes6whgPQ6NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Predictor with Best Weights\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"best_model.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Register Test Set\n",
        "try:\n",
        "    register_coco_instances(\"test_dataset\", {}, \"/content/test/_annotations.coco.json\", \"/content/test\")\n",
        "except AssertionError: pass\n",
        "\n",
        "# Evaluate\n",
        "evaluator = COCOEvaluator(\"test_dataset\", output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"test_dataset\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
      ],
      "metadata": {
        "id": "RgTv8zqdRRrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_city_heatmap(city_name, zip_path, output_csv_name, radius_km=None, zoom=19, tile_size_px=640):\n",
        "    \"\"\"\n",
        "    Generates a solar panel heatmap for a specific city.\n",
        "    1. Downloads/Generates grid coordinates.\n",
        "    2. Unzips image tiles.\n",
        "    3. Runs inference.\n",
        "    4. Saves data and creates a Folium map.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Processing {city_name} ---\")\n",
        "\n",
        "    # 1. Generate Coordinate Map\n",
        "    print(\"üåç Generating Tile-to-Coordinate map...\")\n",
        "    tile_coord_map = {}\n",
        "    try:\n",
        "        if radius_km:\n",
        "            # Radius Buffer Method (e.g. Lucknow)\n",
        "            center_lat, center_lon = ox.geocode(city_name)\n",
        "            df_point = gpd.GeoDataFrame(geometry=[Point(center_lon, center_lat)], crs=\"EPSG:4326\")\n",
        "            polygon = df_point.to_crs(epsg=3857).buffer(radius_km * 1000).to_crs(epsg=4326).geometry.iloc[0]\n",
        "        else:\n",
        "            # Boundary Method (e.g. Jaipur, Chandigarh)\n",
        "            gdf_city = ox.geocode_to_gdf(city_name).to_crs(epsg=4326)\n",
        "            polygon = gdf_city.union_all()\n",
        "\n",
        "        minx, miny, maxx, maxy = polygon.bounds\n",
        "        avg_lat_rad = np.radians((miny + maxy) / 2)\n",
        "        meters_per_pixel = (156543.03 * np.cos(avg_lat_rad)) / (2**zoom)\n",
        "        tile_size_deg = (tile_size_px * meters_per_pixel) / 111320.0\n",
        "\n",
        "        current_index = 0\n",
        "        # Iterate grid\n",
        "        for lat in np.arange(miny, maxy, tile_size_deg):\n",
        "            for lon in np.arange(minx, maxx, tile_size_deg):\n",
        "                # Approximate check if tile is within city/circle\n",
        "                tile_point = Point(lon, lat)\n",
        "                if polygon.intersects(tile_point.buffer(tile_size_deg/2)):\n",
        "                    tile_coord_map[current_index] = (lat, lon)\n",
        "                current_index += 1\n",
        "        print(f\"‚úÖ Mapped {len(tile_coord_map)} potential tile locations.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating map: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Unzip Images\n",
        "    img_folder = f\"/content/{city_name.split(',')[0].replace(' ', '_')}_images\"\n",
        "    if not os.path.exists(img_folder):\n",
        "        print(f\"‚è≥ Unzipping {zip_path}...\")\n",
        "        !unzip -qn \"{zip_path}\" -d \"{img_folder}\"\n",
        "\n",
        "    # Finds the subfolder with images\n",
        "    target_folder = img_folder\n",
        "    for root, dirs, files in os.walk(img_folder):\n",
        "        if any(f.endswith('.png') for f in files):\n",
        "            target_folder = root\n",
        "            break\n",
        "\n",
        "    image_files = sorted(glob.glob(os.path.join(target_folder, \"*.png\")))\n",
        "    print(f\"‚úÖ Found {len(image_files)} images.\")\n",
        "\n",
        "    # 3. Run Inference\n",
        "    print(\"üöÄ Starting Inference...\")\n",
        "    results = []\n",
        "    for img_path in tqdm(image_files):\n",
        "        try:\n",
        "            # Extract ID from filename (assuming tile_123.png format)\n",
        "            tile_id = int(os.path.basename(img_path).replace(\"tile_\", \"\").replace(\".png\", \"\"))\n",
        "        except: continue\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None: continue\n",
        "\n",
        "        outputs = predictor(img)\n",
        "        instances = outputs[\"instances\"].to(\"cpu\")\n",
        "        num_panels = len(instances)\n",
        "\n",
        "        if num_panels > 0:\n",
        "            lat, lon = tile_coord_map.get(tile_id, (None, None))\n",
        "            if lat:\n",
        "                results.append([lat, lon, num_panels])\n",
        "\n",
        "    # 4. Save & Plot\n",
        "    if results:\n",
        "        df = pd.DataFrame(results, columns=['lat', 'lon', 'count'])\n",
        "        df.to_csv(output_csv_name, index=False)\n",
        "        print(f\"üíæ Saved CSV to {output_csv_name}\")\n",
        "\n",
        "        # Generate Map\n",
        "        center_lat = df['lat'].mean()\n",
        "        center_lon = df['lon'].mean()\n",
        "        m = folium.Map(location=[center_lat, center_lon], zoom_start=13, tiles=\"CartoDB dark_matter\")\n",
        "        HeatMap(df.values.tolist(), radius=15, blur=10).add_to(m)\n",
        "\n",
        "        map_path = output_csv_name.replace(\".csv\", \".html\")\n",
        "        m.save(map_path)\n",
        "        print(f\"‚úÖ Heatmap saved to {map_path}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No panels detected or coordinate mapping failed.\")\n",
        "\n",
        "# --- Example Usage for Cities mentioned in notebook ---\n",
        "\n",
        "# 1. Jaipur\n",
        "# generate_city_heatmap(\"Jaipur Municipal Corporation, India\",\n",
        "#                       \"/content/drive/My Drive/Jaipur_Correct_City_Tiles.zip\",\n",
        "#                       \"/content/drive/My Drive/Jaipur_Summary.csv\")\n",
        "\n",
        "# 2. Chandigarh\n",
        "# generate_city_heatmap(\"Chandigarh, India\",\n",
        "#                       \"/content/drive/My Drive/Chandigarh_Correct_City_Tiles.zip\",\n",
        "#                       \"/content/drive/My Drive/Chandigarh_Summary.csv\")\n",
        "\n",
        "# 3. Lucknow (Uses Radius)\n",
        "# generate_city_heatmap(\"Lucknow, India\",\n",
        "#                       \"/content/drive/My Drive/Lucknow_City_Radius_Tiles.zip\",\n",
        "#                       \"/content/drive/My Drive/Lucknow_Summary.csv\",\n",
        "#                       radius_km=14)"
      ],
      "metadata": {
        "id": "LyOAKogjRUPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}